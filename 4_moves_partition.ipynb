{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a13041",
   "metadata": {},
   "source": [
    "# üèÜ Chess Analysis - Data Preparation Pipeline\n",
    "\n",
    "## Notebook Overview\n",
    "This notebook performs the heavy data transformations required for our chess analysis. It takes the clean, game-level Parquet data and creates two specialized, analysis-ready assets:\n",
    "\n",
    "1.  **Move-Level Checkpoint**: A detailed dataset where every single move from every game is its own row.\n",
    "2.  **Player Rating Summary**: A summary CSV file with the average Blitz, Bullet, and Classical rating for every unique player.\n",
    "\n",
    "These assets will serve as fast, efficient inputs for our final visualization notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d176213",
   "metadata": {},
   "source": [
    "---\n",
    "### **Part 1: Setup and Initialization**\n",
    "This section imports all necessary libraries and initializes the Spark Session, which is the entry point for our data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915ae556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, split, explode, regexp_extract, when, substring, avg, round\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29bf870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChessAnalysisPipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "processed_path = \"/home/yash/chess_project/processed/chess_games_parquet\"\n",
    "df_games = spark.read.parquet(processed_path)\n",
    "print(\"--- Game-level data loaded successfully. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bb61ea",
   "metadata": {},
   "source": [
    "---\n",
    "### **Part 2: Create Move-Level Checkpoint**\n",
    "\n",
    "This is a heavy transformation. We will process the `Moves` column for every game, explode it into a massive DataFrame with one row per move, and save the result as an efficient Parquet checkpoint. This allows our final visualizations to run in seconds instead of minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc19b41",
   "metadata": {},
   "source": [
    "#### **Step 2.1: Define All Transformations**\n",
    "All data manipulations are defined in a single, chained command. Spark's lazy evaluation will optimize this entire chain for efficient execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c42122",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_move = df_games \\\n",
    "    .withColumn(\"CleanedMoves\", regexp_replace(col(\"Moves\"), r\"\\{.*?\\}\\s*\", \"\")) \\\n",
    "    .withColumn(\"CleanedMoves\", regexp_replace(col(\"CleanedMoves\"), r\"\\d+\\.+\\s*\", \"\")) \\\n",
    "    .withColumn(\"CleanedMoves\", regexp_replace(col(\"CleanedMoves\"), r\"\\s*(1-0|0-1|1/2-1/2)$\", \"\")) \\\n",
    "    .withColumn(\"CleanedMoves\", regexp_replace(col(\"CleanedMoves\"), r\"\\s+\", \" \")) \\\n",
    "    .withColumn(\"CleanedMoves\", trim(col(\"CleanedMoves\"))) \\\n",
    "    .withColumn(\"MovesArray\", split(col(\"CleanedMoves\"), \" \")) \\\n",
    "    .withColumn(\"Move\", explode(\"MovesArray\")) \\\n",
    "    .withColumn(\"ToSquare\", regexp_extract(col(\"Move\"), r\"([a-h][1-8])\", 1)) \\\n",
    "    .withColumn(\"Piece\",\n",
    "        when(col(\"Move\").like(\"O-O%\"), \"K\")\n",
    "        .when(col(\"Move\").rlike(\"^[KQRBN]\"), substring(col(\"Move\"), 1, 1))\n",
    "        .when(col(\"Move\").rlike(\"^[a-h]\"), \"P\")\n",
    "        .otherwise(\"Unknown\")\n",
    "    ) \\\n",
    "    .filter(col(\"ToSquare\") != \"\")\n",
    "\n",
    "print(\"--- All transformations defined. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642b1e9",
   "metadata": {},
   "source": [
    "#### **Step 2.2: Save the Checkpoint**\n",
    "To avoid memory errors on this large dataset, we write the checkpoint one partition at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda7fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Creating the lean checkpoint... ---\")\n",
    "lean_df = df_per_move.select(\n",
    "    \"EventType\", \"WhiteElo\", \"BlackElo\",\n",
    "    \"Move\", \"Piece\", \"ToSquare\"\n",
    ")\n",
    "\n",
    "checkpoint_path = \"processed/final_moves_checkpoint.parquet\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    shutil.rmtree(checkpoint_path)\n",
    "\n",
    "event_types = [row.EventType for row in lean_df.select(\"EventType\").distinct().collect()]\n",
    "\n",
    "for event in event_types:\n",
    "    print(f\"Writing checkpoint for EventType: {event}\")\n",
    "    df_one_event = lean_df.filter(col(\"EventType\") == event)\n",
    "    partition_path = os.path.join(checkpoint_path, f\"EventType={event}\")\n",
    "    df_one_event.drop(\"EventType\").write.mode(\"overwrite\").parquet(partition_path)\n",
    "\n",
    "with open(os.path.join(checkpoint_path, \"_SUCCESS\"), 'w') as f: pass\n",
    "print(f\"--- Checkpoint saved successfully to {checkpoint_path} ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d0a85",
   "metadata": {},
   "source": [
    "#### **Step 2.3: Quick Verification (Optional)**\n",
    "As a final check, we can perform a quick aggregation on the transformed data to ensure it's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7976ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Aggregating move counts for heatmap... ---\")\n",
    "heatmap_data_pd = df_per_move \\\n",
    "    .groupBy(\"ToSquare\", \"Piece\") \\\n",
    "    .count() \\\n",
    "    .toPandas()\n",
    "\n",
    "print(\"--- Aggregation complete! ---\")\n",
    "print(\"Top 20 most frequent destinations:\")\n",
    "print(heatmap_data_pd.sort_values(by=\"count\", ascending=False).head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af60cd",
   "metadata": {},
   "source": [
    "---\n",
    "### **Part 3: Create Player Rating Summary**\n",
    "This section performs another heavy aggregation. It processes all games to calculate the average Blitz, Bullet, and Classical rating for every unique player, saving the final summary to a single, portable CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Aggregating ratings for each player... (This may take several minutes) ---\")\n",
    "player_ratings_df = df_games.select(col(\"White\").alias(\"Player\"), col(\"EventType\"), col(\"WhiteElo\").alias(\"Rating\")) \\\n",
    "    .union(df_games.select(col(\"Black\").alias(\"Player\"), col(\"EventType\"), col(\"BlackElo\").alias(\"Rating\")))\n",
    "\n",
    "player_ratings_filtered = player_ratings_df.filter(col(\"EventType\").isin([\"blitz\", \"bullet\", \"classical\"]))\n",
    "\n",
    "player_ratings_wide = player_ratings_filtered.groupBy(\"Player\").pivot(\"EventType\", [\"blitz\", \"bullet\", \"classical\"]).agg(avg(\"Rating\"))\n",
    "\n",
    "player_ratings_cleaned = player_ratings_wide \\\n",
    "    .withColumn(\"blitz_rating\", round(col(\"blitz\"), 0)) \\\n",
    "    .withColumn(\"bullet_rating\", round(col(\"bullet\"), 0)) \\\n",
    "    .withColumn(\"classical_rating\", round(col(\"classical\"), 0)) \\\n",
    "    .select(\"Player\", \"blitz_rating\", \"bullet_rating\", \"classical_rating\") \\\n",
    "    .na.drop(how=\"all\", subset=[\"blitz_rating\", \"bullet_rating\", \"classical_rating\"])\n",
    "\n",
    "player_ratings_cleaned.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"player_analysis_results/player_ratings.csv\")\n",
    "\n",
    "print(\"--- Player ratings CSV saved successfully! ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_env)",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
