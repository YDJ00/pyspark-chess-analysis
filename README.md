End-to-End Spark Chess Analysis Pipeline

This repository contains a complete data engineering and analysis project that processes and visualizes a 7.5 GB dataset of standard rated chess games from Lichess. The entire pipeline is built using PySpark to handle the large-scale data, with final visualizations created using Plotly for interactivity.



This project demonstrates a robust, end-to-end workflow, from raw data ingestion and cleaning to advanced analysis and the creation of a final insights dashboard.



Key Project Features

Large-Scale Data Processing: Efficiently processes a 7.5 GB PGN file on a memory-constrained system (8 GB RAM) using Apache Spark.



Robust ETL Pipeline: Implements a multi-stage ETL process to parse, clean, normalize, and structure the raw game data.



Optimized Storage: Saves processed data in a partitioned Parquet format, enabling fast and efficient queries for analysis.



Advanced Analytics: Explores various aspects of the dataset, including player rating distributions, opening theory, endgame patterns, and performance correlations across time controls.



Interactive Visualizations: Creates a full suite of aesthetic and interactive data visualizations using Plotly to present the findings in a clear and engaging "dashboard" format.



Professional Workflow: The project is structured into modular notebooks for data acquisition, processing, and visualization, following best practices for data engineering projects.



Sample Visualizations and Insights

Here are a few examples of the interactive plots generated by the analysis pipeline.



Player ELO Rating Distribution

This histogram shows the overall skill distribution of all players in the dataset, highlighting a concentration of players in the 1400-1800 ELO range.



Checkmate Heatmap

This heatmap reveals the most common squares on the board where a Queen delivers a checkmate, providing insights into critical endgame patterns.



Interactive Opening Tree

This sunburst chart provides an interactive way to explore the most common opening move sequences in the dataset.



Technical Stack

Data Processing: Apache Spark (PySpark)



Data Analysis: Pandas, NumPy



Visualization: Plotly, Matplotlib, Seaborn



Development Environment: Jupyter Lab, WSL (Ubuntu)



Core Language: Python



Project Structure

The project is organized into a series of notebooks, each with a specific purpose, to create a clean and reproducible pipeline:



01\_Acquisition.ipynb: Downloads and decompresses the raw data.



02\_ETL\_Process\_Games.ipynb: The main ETL pipeline that processes the raw PGN file into a clean, game-level Parquet dataset.



03\_Create\_Analysis\_Assets.ipynb: Performs heavy, secondary transformations to create specialized checkpoints for analysis (e.g., the move-level dataset and the player rating summary).



04\_Visualization\_Dashboard.ipynb: The final notebook that loads the fast, clean assets to generate all the plots and statistical summaries.



How to Run This Project

Clone the Repository



git clone \[https://github.com/YDJ00/pyspark-chess-analysis.git](https://github.com/YDJ00/pyspark-chess-analysis.git)

cd pyspark-chess-analysis

Set Up the Environment

Create a Python virtual environment and install the required libraries.



python -m venv spark\_env

source spark\_env/bin/activate

pip install -r requirements.txt

Run the Notebooks

Launch Jupyter Lab and run the notebooks in the following order to execute the full pipeline:



01\_Acquisition.ipynb



02\_ETL\_Process\_Games.ipynb



03\_Create\_Analysis\_Assets.ipynb



04\_Visualization\_Dashboard.ipynb



Last updated: Fri, 26 Sep 2025 11:11:20 +0530 â€” small README polish
